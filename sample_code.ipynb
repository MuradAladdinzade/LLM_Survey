{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BytePairEncoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial tokens: ['T', 'h', 'i', 's', '_', ' ', 'i', 's', '_', ' ', 'a', '_', ' ', 's', 'i', 'm', 'p', 'l', 'e', '_', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', '_', ' ', 't', 'o', '_', ' ', 'd', 'e', 'm', 'o', 'n', 's', 't', 'r', 'a', 't', 'e', '_', ' ', 'B', 'y', 't', 'e', 'P', 'a', 'i', 'r', 'E', 'n', 'c', 'o', 'd', 'i', 'n', 'g', '.', '_', ' ', 'B', 'y', 't', 'e', 'P', 'a', 'i', 'r', 'E', 'n', 'c', 'o', 'd', 'i', 'n', 'g', '_', ' ', 'i', 's', '_', ' ', 'e', 'f', 'f', 'e', 'c', 't', 'i', 'v', 'e', '.']\n",
      "Merge #1: ('_', ' ') -> _ \n",
      "Merge #2: ('_', ' ') -> _ \n",
      "Merge #3: ('_', ' ') -> _ \n",
      "Merge #4: ('_', ' ') -> _ \n",
      "Merge #5: ('_', ' ') -> _ \n",
      "Tokens after BPE: ['T', 'h', 'i', 's', '_', ' ', 'i', 's', '_', ' ', 'a', '_', ' ', 's', 'i', 'm', 'p', 'l', 'e', '_', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', '_', ' ', 't', 'o', '_', ' ', 'd', 'e', 'm', 'o', 'n', 's', 't', 'r', 'a', 't', 'e', '_', ' ', 'B', 'y', 't', 'e', 'P', 'a', 'i', 'r', 'E', 'n', 'c', 'o', 'd', 'i', 'n', 'g', '.', '_', ' ', 'B', 'y', 't', 'e', 'P', 'a', 'i', 'r', 'E', 'n', 'c', 'o', 'd', 'i', 'n', 'g', '_', ' ', 'i', 's', '_', ' ', 'e', 'f', 'f', 'e', 'c', 't', 'i', 'v', 'e', '.']\n"
     ]
    }
   ],
   "source": [
    "#### BytePairEncoding (BPE)\n",
    "\n",
    "# Example text for encoding\n",
    "text = \"This is a simple example to demonstrate BytePairEncoding. BytePairEncoding is effective.\"\n",
    "\n",
    "# Mock implementation of BytePairEncoding\n",
    "def byte_pair_encoding(text, num_merges=10):\n",
    "    from collections import Counter, defaultdict\n",
    "\n",
    "    # Tokenize the text into symbols\n",
    "    tokens = list(text.replace(\" \", \"_ \")) # Use '_' to denote space (word boundaries)\n",
    "    print(\"Initial tokens:\", tokens)\n",
    "\n",
    "    # Count frequency of pairs\n",
    "    def get_stats(tokens):\n",
    "        pairs = Counter(zip(tokens[:-1], tokens[1:]))\n",
    "        return pairs\n",
    "\n",
    "    # Merge function\n",
    "    def merge_vocab(pair, v_in):\n",
    "        v_out = []\n",
    "        bigram = ' '.join(pair)\n",
    "        replacement = ''.join(pair)\n",
    "        for word in v_in:\n",
    "            if bigram in word:\n",
    "                word = word.replace(bigram, replacement)\n",
    "            v_out.append(word)\n",
    "        return v_out\n",
    "\n",
    "    # Perform num_merges iterations to merge frequent pairs\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(tokens)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        tokens = merge_vocab(best, tokens)\n",
    "        print(f\"Merge #{i+1}: {best} -> {''.join(best)}\")\n",
    "\n",
    "    # Final tokens after merges\n",
    "    print(\"Tokens after BPE:\", tokens)\n",
    "\n",
    "# Run the BPE example\n",
    "byte_pair_encoding(text, num_merges=5)\n",
    "\n",
    "# Note: This is a simplified example. In practice, BPE computes frequencies over a large corpus and merges iteratively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordPieceEncoding\n",
    "\n",
    "### This very simple example, in fact it requires huge compute power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Vocabulary: Counter({'e': 13, ' ': 10, 'i': 9, 'o': 7, 'n': 7, 's': 5, 'm': 5, 'd': 5, 'c': 5, '_': 4, 'a': 4, 't': 4, 'r': 4, 'p': 2, 'l': 2, 'x': 2, 'W': 2, 'P': 2, 'E': 2, 'g': 2, '.': 2, 'T': 1, 'h': 1, 'z': 1, 'k': 1, 'f': 1, 'q': 1, 'u': 1, 'y': 1})\n",
      "WordPiece Tokens: ['_', 'W', 'o', 'r', 'd', 'P', 'i', 'e', 'c', 'e', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', '.', '_']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenize the text into a sequence of characters, including spaces.\"\"\"\n",
    "    return [\"_\"] + list(text) + [\"_\"]\n",
    "\n",
    "def build_vocab(texts):\n",
    "    \"\"\"Build initial vocabulary from input texts.\"\"\"\n",
    "    vocab = Counter()\n",
    "    for text in texts:\n",
    "        tokens = tokenize(text)\n",
    "        vocab.update(tokens)\n",
    "    return vocab\n",
    "\n",
    "def find_best_pair(vocab):\n",
    "    \"\"\"Find the most frequent pair of tokens in the vocabulary.\"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for token, freq in vocab.items():\n",
    "        symbols = token.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[(symbols[i], symbols[i+1])] += freq\n",
    "    return max(pairs, key=pairs.get) if pairs else None\n",
    "\n",
    "def merge_vocab(pair, vocab):\n",
    "    \"\"\"Merge the most frequent pair in the vocabulary.\"\"\"\n",
    "    new_vocab = {}\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement = ''.join(pair)\n",
    "    pattern = re.escape(bigram)\n",
    "    replacement = replacement.replace(' ', '')\n",
    "    for word in vocab:\n",
    "        new_word = re.sub(pattern, replacement, word)\n",
    "        new_vocab[new_word] = vocab[word]\n",
    "    return new_vocab\n",
    "\n",
    "def wordpiece_tokenize(vocab, text, num_merges=100):\n",
    "    \"\"\"Tokenize input text based on the WordPiece vocabulary.\"\"\"\n",
    "    for _ in range(num_merges):\n",
    "        pair = find_best_pair(vocab)\n",
    "        if not pair:\n",
    "            break\n",
    "        vocab = merge_vocab(pair, vocab)\n",
    "    # Tokenize the text based on the final vocabulary\n",
    "    tokens = tokenize(text)\n",
    "    wordpiece_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            wordpiece_tokens.append(token)\n",
    "        else:\n",
    "            wordpiece_tokens.append(\"[UNK]\")\n",
    "    return wordpiece_tokens\n",
    "\n",
    "# Example usage\n",
    "texts = [\"This is a simple example to demonstrate WordPieceEncoding.\",\n",
    "         \"WordPieceEncoding maximizes token frequency.\"]\n",
    "vocab = build_vocab(texts)\n",
    "print(\"Initial Vocabulary:\", vocab)\n",
    "\n",
    "# Tokenize a new text using the WordPiece vocabulary\n",
    "new_text = \"WordPiece tokenization example.\"\n",
    "wordpiece_tokens = wordpiece_tokenize(vocab, new_text, num_merges=10)\n",
    "print(\"WordPiece Tokens:\", wordpiece_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\murad aladdinzada\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentencePieceEncoding\n",
    "\n",
    "### Again, this is very simple example to show SentencePieceEncoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Pieces: ['▁', 'S', 'en', 't', 'en', 'ce', 'Pi', 'e', 'ce', '▁', 'en', 'co', 'd', 'i', 'n', 'g', '▁', 'i', 's', '▁', 'f', 'a', 's', 'c', 'i', 'n', 'a', 't', 'i', 'n', 'g', '.']\n",
      "Encoded IDs: [10, 22, 3, 14, 3, 4, 7, 9, 4, 10, 3, 6, 15, 16, 11, 30, 10, 16, 18, 10, 0, 17, 18, 12, 16, 11, 17, 14, 16, 11, 30, 20]\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "# Prepare a sample text\n",
    "sample_text = \"This is a simple example to demonstrate how SentencePiece encoding works. SentencePiece can encode and decode text.\"\n",
    "text_file = \"sample_text.txt\"\n",
    "\n",
    "with open(text_file, \"w\") as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "# Train the SentencePiece model\n",
    "spm.SentencePieceTrainer.Train(f'--input={text_file} --model_prefix=sentencepiece_model --vocab_size=32 --model_type=bpe')\n",
    "\n",
    "# Load the trained SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"sentencepiece_model.model\")\n",
    "\n",
    "# Encode a text string\n",
    "text_to_encode = \"SentencePiece encoding is fascinating.\"\n",
    "encoded_pieces = sp.EncodeAsPieces(text_to_encode)\n",
    "encoded_ids = sp.EncodeAsIds(text_to_encode)\n",
    "\n",
    "print(\"Encoded Pieces:\", encoded_pieces)\n",
    "print(\"Encoded IDs:\", encoded_ids)\n",
    "\n",
    "# Cleanup the generated files\n",
    "os.remove(text_file)\n",
    "os.remove(\"sentencepiece_model.model\")\n",
    "os.remove(\"sentencepiece_model.vocab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
